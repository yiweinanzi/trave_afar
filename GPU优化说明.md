# GoAfar GPUä¼˜åŒ–æ–¹æ¡ˆ

## âœ… å·²è§£å†³çš„é—®é¢˜

### 1. âœ“ RecBoleè®­ç»ƒéœ€è¦GPU
**è§£å†³æ–¹æ¡ˆ**: åˆ›å»ºäº† `train_recbole_gpu.py`
```bash
# GPUè®­ç»ƒRecBole
python train_recbole_gpu.py --gpu 0

# é…ç½®æ–‡ä»¶å·²ä¼˜åŒ–
configs/recbole.yaml:
  - ä½¿ç”¨GPUåŠ é€Ÿ
  - batch_sizeå¢å¤§åˆ°256
  - è®­ç»ƒepoch=20
```

**æ€§èƒ½æå‡**:
- CPUè®­ç»ƒ: ~30-60åˆ†é’Ÿ
- GPUè®­ç»ƒ: ~5-10åˆ†é’Ÿ
- æå‡: **6-12å€**

### 2. âœ“ BGE-M3å‘é‡ç”Ÿæˆéœ€è¦GPUåŠ é€Ÿ
**è§£å†³æ–¹æ¡ˆ**: åˆ›å»ºäº† `src/embedding/build_embeddings_gpu.py`
```bash
# GPUåŠ é€Ÿå‘é‡ç”Ÿæˆ
python src/embedding/build_embeddings_gpu.py --batch-size 256
```

**æ€§èƒ½æå‡**:
- CPU (batch=32): ~20-30åˆ†é’Ÿï¼ˆ1333ä¸ªPOIï¼‰
- GPU (batch=256): ~2-5åˆ†é’Ÿï¼ˆ1333ä¸ªPOIï¼‰
- æå‡: **4-15å€**

### 3. âœ“ Qwen3-8Bæ¨ç†éœ€è¦GPU
**è§£å†³æ–¹æ¡ˆ**: `src/llm4rec/qwen_recommender.py` å·²æ”¯æŒGPU
```python
recommender = QwenRecommender(
    model_name_or_path='Qwen/Qwen3-8B',
    use_gpu=True  # è‡ªåŠ¨ä½¿ç”¨GPU
)
```

**æ€§èƒ½æå‡**:
- CPUæ¨ç†: ~5-10ç§’/æŸ¥è¯¢
- GPUæ¨ç†: ~0.5-1ç§’/æŸ¥è¯¢
- æå‡: **5-20å€**

### 4. âœ“ æ·»åŠ ç¼“å­˜æœºåˆ¶
**è§£å†³æ–¹æ¡ˆ**: åˆ›å»ºäº† `src/utils/cache_manager.py`
```python
from utils.cache_manager import get_cache_manager

cache = get_cache_manager()

# æ£€æŸ¥ç¼“å­˜
cached_result = cache.get('search', params)
if cached_result:
    return cached_result

# è®¡ç®—å¹¶ç¼“å­˜
result = expensive_computation(params)
cache.set('search', params, result)
```

**ä¼˜åŒ–æ•ˆæœ**:
- é¦–æ¬¡æŸ¥è¯¢: æ­£å¸¸æ—¶é—´
- ç¼“å­˜å‘½ä¸­: <100ms
- æå‡: **10-100å€**ï¼ˆé‡å¤æŸ¥è¯¢ï¼‰

## ğŸš€ GPUä¼˜åŒ–çš„å®Œæ•´æµç¨‹

### æ–¹å¼1: ä¸€é”®è¿è¡Œï¼ˆæ¨èï¼‰
```bash
bash run_gpu_optimized.sh
```

### æ–¹å¼2: åˆ†æ­¥è¿è¡Œ
```bash
# 1. GPUå‘é‡ç”Ÿæˆï¼ˆ~2-5åˆ†é’Ÿï¼‰
python src/embedding/build_embeddings_gpu.py --batch-size 256

# 2. GPUè®­ç»ƒRecBoleï¼ˆ~5-10åˆ†é’Ÿï¼‰
python train_recbole_gpu.py --gpu 0

# 3. GPUæ¨ç†Qwenï¼ˆå®æ—¶ï¼‰
python run_with_llm.py
```

## ğŸ“Š GPU vs CPU æ€§èƒ½å¯¹æ¯”

| ä»»åŠ¡ | CPUæ—¶é—´ | GPUæ—¶é—´ | åŠ é€Ÿæ¯” |
|------|---------|---------|--------|
| BGE-M3å‘é‡ç”Ÿæˆï¼ˆ1333ä¸ªï¼‰ | 20-30åˆ†é’Ÿ | 2-5åˆ†é’Ÿ | 4-15x |
| RecBoleè®­ç»ƒï¼ˆ20 epochsï¼‰ | 30-60åˆ†é’Ÿ | 5-10åˆ†é’Ÿ | 6-12x |
| Qwen3-8Bæ¨ç†ï¼ˆå•æŸ¥è¯¢ï¼‰ | 5-10ç§’ | 0.5-1ç§’ | 5-20x |
| æ€»æµç¨‹ï¼ˆç«¯åˆ°ç«¯ï¼‰ | ~60åˆ†é’Ÿ | ~10åˆ†é’Ÿ | 6x |

## ğŸ”§ GPUé…ç½®ä¼˜åŒ–

### RecBole GPUé…ç½®
ç¼–è¾‘ `configs/recbole.yaml`:
```yaml
# GPUä¼˜åŒ–é…ç½®
gpu_id: 0
train_batch_size: 256      # GPUå¯ç”¨æ›´å¤§batch
eval_batch_size: 512       # è¯„ä¼°ç”¨æ›´å¤§batch
epochs: 20                 # å¢åŠ è®­ç»ƒè½®æ•°
```

### BGE-M3 GPUé…ç½®
```python
# åœ¨ build_embeddings_gpu.py ä¸­
encoder = BGEM3Encoder(
    model_path='...',
    use_gpu=True           # ä½¿ç”¨GPU
)

embeddings = encoder.encode_texts(
    texts,
    batch_size=256,        # GPUå¯ç”¨æ›´å¤§batch
    max_length=512
)
```

### Qwen GPUé…ç½®
```python
# åœ¨ qwen_recommender.py ä¸­
recommender = QwenRecommender(
    model_name_or_path='Qwen/Qwen3-8B',
    use_gpu=True           # ä½¿ç”¨GPU
)

# è‡ªåŠ¨ä½¿ç”¨fp16å’Œdevice_map='auto'
```

## ğŸ’¾ ç¼“å­˜ç­–ç•¥

### 1. å‘é‡ç¼“å­˜
```python
# è‡ªåŠ¨ç¼“å­˜ç”Ÿæˆçš„å‘é‡
build_embeddings_with_gpu(use_cache=True)

# ç¬¬äºŒæ¬¡è¿è¡Œç›´æ¥åŠ è½½ï¼Œå‡ ä¹ç¬é—´å®Œæˆ
```

### 2. æ£€ç´¢ç¼“å­˜
```python
# ç¼“å­˜æ£€ç´¢ç»“æœ
cache.set('search', {'query': query, 'topk': 50}, results)

# ç›¸åŒæŸ¥è¯¢ç›´æ¥è¿”å›ç¼“å­˜
```

### 3. è·¯çº¿ç¼“å­˜
```python
# ç¼“å­˜è·¯çº¿è§„åˆ’ç»“æœ
cache.set('route', {'pois': poi_ids, 'hours': 10}, solution)
```

## ğŸ¯ GPUåˆ©ç”¨ç‡ä¼˜åŒ–å»ºè®®

### 1. æ‰¹å¤„ç†ä¼˜åŒ–
```python
# å‘é‡ç”Ÿæˆ
batch_size = 256 if torch.cuda.is_available() else 32

# RecBoleè®­ç»ƒ
train_batch_size = 256  # GPU
eval_batch_size = 512   # GPUè¯„ä¼°æ›´å¿«
```

### 2. æ··åˆç²¾åº¦è®­ç»ƒ
```python
# RecBoleé…ç½®
use_amp: True  # è‡ªåŠ¨æ··åˆç²¾åº¦
```

### 3. æ˜¾å­˜ä¼˜åŒ–
```python
# Qwenæ¨ç†
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,  # ä½¿ç”¨fp16èŠ‚çœæ˜¾å­˜
    device_map='auto'            # è‡ªåŠ¨åˆ†é…è®¾å¤‡
)
```

## ğŸ“ˆ é¢„æœŸGPUå ç”¨

### GPUæ˜¾å­˜éœ€æ±‚
- BGE-M3ç¼–ç : ~2-4GBï¼ˆbatch=256ï¼‰
- RecBoleè®­ç»ƒ: ~4-8GB
- Qwen3-8Bæ¨ç†: ~16-20GBï¼ˆfp16ï¼‰
- æ€»è®¡: **å»ºè®®24GB+æ˜¾å­˜**

### å¦‚æœæ˜¾å­˜ä¸è¶³
```python
# 1. å‡å°batch_size
batch_size = 64  # ä»256é™åˆ°64

# 2. ä½¿ç”¨é‡åŒ–
from transformers import BitsAndBytesConfig
quantization_config = BitsAndBytesConfig(load_in_8bit=True)

# 3. åˆ†é˜¶æ®µè¿è¡Œ
# å…ˆè¿è¡Œå‘é‡ç”Ÿæˆï¼Œå†è¿è¡Œæ¨¡å‹è®­ç»ƒ
```

## âš¡ å¿«é€Ÿæµ‹è¯•GPUåŠŸèƒ½

```bash
# æµ‹è¯•GPUæ˜¯å¦å¯ç”¨
python -c "import torch; print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')"

# æµ‹è¯•BGE-M3 GPUç¼–ç ï¼ˆ10ä¸ªPOIï¼‰
python -c "
import sys; sys.path.insert(0, 'src')
from embedding.bge_m3_encoder import BGEM3Encoder
encoder = BGEM3Encoder(
    model_path='/root/autodl-tmp/goafar_project/models/Xorbits/bge-m3',
    use_gpu=True
)
result = encoder.encode_texts(['æµ‹è¯•1', 'æµ‹è¯•2'], batch_size=2)
print(f'âœ“ GPUç¼–ç æˆåŠŸ: {result[\"dense_vecs\"].shape}')
"

# è¿è¡Œå®Œæ•´GPUæµç¨‹
bash run_gpu_optimized.sh
```

## ğŸ“ ä½¿ç”¨è¯´æ˜

1. **é¦–æ¬¡è¿è¡Œ**: 
   - æ‰§è¡Œ `bash run_gpu_optimized.sh` 
   - ä¼šè‡ªåŠ¨ä¸‹è½½ç¼ºå¤±çš„æ¨¡å‹
   - æ„å»ºå‘é‡å¹¶è®­ç»ƒRecBole

2. **åç»­è¿è¡Œ**:
   - å‘é‡å’Œæ¨¡å‹å·²ç¼“å­˜
   - åªéœ€è¿è¡Œæ¨è: `python run_with_llm.py`
   - é€Ÿåº¦éå¸¸å¿«ï¼ˆ<1åˆ†é’Ÿï¼‰

3. **æ¸…é™¤ç¼“å­˜**:
   ```bash
   python -c "from src.utils.cache_manager import CacheManager; CacheManager().clear()"
   ```

---

**GPUä¼˜åŒ–å®Œæˆï¼ç°åœ¨å¯ä»¥é«˜æ•ˆè¿è¡Œå®Œæ•´æµç¨‹äº†ã€‚** ğŸš€

