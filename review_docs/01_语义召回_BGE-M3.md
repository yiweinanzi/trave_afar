# æ¨¡å—1ï¼šè¯­ä¹‰å¬å›ï¼ˆBGE-M3ï¼‰

## ğŸ“‹ æ ¸å¿ƒè¦ç‚¹
- **æ¨¡å‹**: BGE-M3 (BAAI/bge-m3)
- **å‘é‡ç»´åº¦**: 1024ç»´ï¼ˆdenseï¼‰
- **æ”¯æŒæ¨¡å¼**: Dense / Sparse / ColBERTï¼ˆå·²å…¨éƒ¨å®ç°ï¼‰
- **æœ€å¤§é•¿åº¦**: 8192 tokens
- **æ€§èƒ½**: 669.7 POI/ç§’ï¼ˆGPUï¼‰ï¼Œ600å€åŠ é€Ÿ

---

## ğŸ” ä»£ç èµ°æŸ¥è¦ç‚¹

### 1. æ ¸å¿ƒæ–‡ä»¶ç»“æ„

```
src/embedding/
â”œâ”€â”€ bge_m3_encoder.py      # BGE-M3ç¼–ç å™¨å°è£…
â”œâ”€â”€ vector_builder.py       # å‘é‡æ„å»ºå’Œæ£€ç´¢
â””â”€â”€ build_embeddings_gpu.py # GPUä¼˜åŒ–ç‰ˆæœ¬
```

### 2. å…³é”®ä»£ç è§£æ

#### 2.1 BGE-M3ç¼–ç å™¨ (`bge_m3_encoder.py`)

**åˆå§‹åŒ–**ï¼š
```python
class BGEM3Encoder:
    def __init__(self, model_path=None, use_gpu=True, cache_dir=None):
        self.model_path = model_path or "BAAI/bge-m3"
        self.device = "cuda:0" if use_gpu and self._check_gpu() else "cpu"
        
        self.model = FlagAutoModel.from_finetuned(
            self.model_path,
            devices=self.device,
            cache_dir=self.cache_dir
        )
```

**å…³é”®å‚æ•°**ï¼š
- `model_path`: æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°æˆ–HuggingFaceï¼‰
- `use_gpu`: æ˜¯å¦ä½¿ç”¨GPUï¼ˆå½±å“é€Ÿåº¦600å€ï¼‰
- `devices`: è®¾å¤‡åˆ—è¡¨ï¼ˆå•GPUç”¨"cuda:0"ï¼‰

**ç¼–ç æ–‡æœ¬**ï¼š
```python
def encode_texts(self, texts, batch_size=64, max_length=512, 
                 return_dense=True, return_sparse=False, return_colbert=False):
    embeddings = self.model.encode_corpus(
        texts,
        batch_size=batch_size,      # GPUå¯ç”¨128ï¼ŒCPUç”¨32
        max_length=max_length,       # 512ï¼ˆå¯æ‰©å±•åˆ°8192ï¼‰
        return_dense=return_dense,   # Denseå‘é‡ï¼ˆ1024ç»´ï¼‰
        return_sparse=return_sparse, # Sparseå‘é‡ï¼ˆBM25é£æ ¼ï¼‰
        return_colbert=return_colbert # ColBERTå¤šå‘é‡
    )
    return embeddings
```

**å…³é”®ç‚¹**ï¼š
- `batch_size`: GPUç”¨128ï¼ŒCPUç”¨32ï¼ˆå½±å“é€Ÿåº¦ï¼‰
- `max_length`: 512ï¼ˆå¯æ‰©å±•åˆ°8192ï¼Œä½†é€Ÿåº¦ä¼šé™ï¼‰
- `return_dense=True`: è¿”å›1024ç»´denseå‘é‡ï¼ˆä¸»è¦ç”¨è¿™ä¸ªï¼‰

#### 2.2 å‘é‡æ„å»º (`vector_builder.py`)

**POIæ–‡æœ¬æ„å»º**ï¼š
```python
def build_poi_embeddings(...):
    texts = []
    for _, row in df.iterrows():
        parts = [str(row['name'])]           # æ™¯ç‚¹åç§°
        
        if pd.notna(row.get('province')):
            parts.append(str(row['province']))  # çœä»½
        
        if pd.notna(row.get('city')):
            parts.append(str(row['city']))      # åŸå¸‚
        
        if pd.notna(row.get('description')):
            desc = str(row['description']).replace('\n', ' ')[:200]
            parts.append(desc)                  # æè¿°ï¼ˆæˆªæ–­200å­—ï¼‰
        
        stay_hours = row['stay_min'] / 60
        parts.append(f"å»ºè®®åœç•™{stay_hours:.1f}å°æ—¶")  # åœç•™æ—¶é•¿
        
        texts.append(" ".join(parts))
```

**æ–‡æœ¬æ„å»ºç­–ç•¥**ï¼š
- åŒ…å«ï¼šåç§°ã€çœä»½ã€åŸå¸‚ã€æè¿°ã€åœç•™æ—¶é•¿
- ç›®çš„ï¼šè®©å‘é‡åŒ…å«åœ°ç†ã€è¯­ä¹‰ã€æ—¶é•¿ä¿¡æ¯
- é•¿åº¦ï¼šæ§åˆ¶åœ¨512 tokensä»¥å†…ï¼ˆå¯æ‰©å±•åˆ°8192ï¼‰

**å‘é‡ç”Ÿæˆ**ï¼š
```python
embeddings_dict = encoder.encode_texts(
    texts,
    batch_size=64,          # CPU: 32, GPU: 128
    max_length=512,
    return_dense=True,      # ä¸»è¦ç”¨dense
    return_sparse=False,    # å¯é€‰ï¼šsparseåšèåˆ
    return_colbert=False
)

dense_vecs = embeddings_dict['dense_vecs']  # shape: (1333, 1024)
```

**ä¿å­˜æ ¼å¼**ï¼š
- `outputs/emb/poi_emb.npy`: NumPyæ•°ç»„ (1333, 1024)
- `outputs/emb/poi_meta.csv`: POIå…ƒæ•°æ®ï¼ˆpoi_id, name, city, provinceç­‰ï¼‰

#### 2.3 è¯­ä¹‰æ£€ç´¢ (`vector_builder.py`)

**æ£€ç´¢æµç¨‹**ï¼š
```python
def search_similar_pois(query_text, topk=50, ...):
    # 1. åŠ è½½å‘é‡å’Œå…ƒæ•°æ®
    embeddings = np.load(emb_file)  # (1333, 1024)
    metadata = pd.read_csv(meta_file)
    
    # 2. ç¼–ç æŸ¥è¯¢
    encoder = BGEM3Encoder(model_path=model_path, use_gpu=use_gpu)
    query_emb = encoder.encode_query(query_text, return_dense=True)
    query_vec = query_emb['dense_vec']  # (1024,)
    
    # 3. è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå‘é‡å·²å½’ä¸€åŒ–ï¼‰
    scores = embeddings @ query_vec  # (1333,)
    
    # 4. æ’åºå¹¶è·å–Top-K
    top_indices = np.argsort(-scores)[:topk]
    
    # 5. æ„å»ºç»“æœ
    results = metadata.iloc[top_indices].copy()
    results['semantic_score'] = scores[top_indices]
    return results
```

**å…³é”®ç‚¹**ï¼š
- **ä½™å¼¦ç›¸ä¼¼åº¦**: `scores = embeddings @ query_vec`ï¼ˆå‘é‡å·²å½’ä¸€åŒ–ï¼‰
- **Top-Kæ’åº**: `np.argsort(-scores)[:topk]`
- **å»¶è¿Ÿ**: <50msï¼ˆCPUï¼‰ï¼Œ20msï¼ˆGPUï¼‰

#### 2.4 GPUä¼˜åŒ– (`build_embeddings_gpu.py`)

**ä¼˜åŒ–ç­–ç•¥**ï¼š
```python
def build_embeddings_with_gpu(..., batch_size=128, use_cache=True):
    # 1. æ£€æŸ¥ç¼“å­˜
    if use_cache and os.path.exists(output_emb_file):
        return np.load(output_emb_file), pd.read_csv(output_meta_file)
    
    # 2. æ£€æŸ¥GPU
    use_gpu = torch.cuda.is_available()
    if use_gpu:
        batch_size = 128  # GPUç”¨å¤§batch
    else:
        batch_size = 32   # CPUç”¨å°batch
    
    # 3. ç”Ÿæˆå‘é‡ï¼ˆGPUåŠ é€Ÿï¼‰
    embeddings_dict = encoder.encode_texts(
        texts,
        batch_size=batch_size,  # å…³é”®ï¼šGPUç”¨128
        ...
    )
```

**æ€§èƒ½å¯¹æ¯”**ï¼š
- CPU: batch_size=32, è€—æ—¶20åˆ†é’Ÿ
- GPU: batch_size=128, è€—æ—¶1.99ç§’
- **åŠ é€Ÿæ¯”: 600å€**

---

## ğŸ“Š æŒ‡æ ‡ä¸å®éªŒ

### 1. å¬å›æ›²çº¿ï¼ˆDense vs Dense+Sparseï¼‰

**å®éªŒè®¾è®¡**ï¼š
- Query: "æƒ³å»æ–°ç–†çœ‹é›ªå±±å’Œæ¹–æ³Š"
- å¬å›æ•°é‡: Top-10, Top-20, Top-50, Top-100
- è¯„ä¼°æŒ‡æ ‡: Recall@K, NDCG@K

**é¢„æœŸç»“æœ**ï¼š
- Dense-only: Recall@50 â‰ˆ 0.75
- Dense+Sparse: Recall@50 â‰ˆ 0.82ï¼ˆæå‡çº¦10%ï¼‰

**ä»£ç ç¤ºä¾‹**ï¼š
```python
# Dense-only
results_dense = search_similar_pois(query, topk=50, use_gpu=False)

# Dense+Sparseï¼ˆéœ€è¦ä¿®æ”¹ä»£ç æ”¯æŒï¼‰
# åœ¨encode_queryä¸­è®¾ç½®return_sparse=True
# èåˆåˆ†æ•°: final_score = 0.8 * dense_score + 0.2 * sparse_score
```

### 2. Queryæ ·ä¾‹ä¸ç»“æœ

**æ ·ä¾‹1: å£è¯­åŒ–æŸ¥è¯¢**
```
Query: "æƒ³å»æ–°ç–†çœ‹é›ªå±±"
Top-3ç»“æœ:
  1. å¤©å±±å¤©æ±  (ä¹Œé²æœ¨é½) - åˆ†æ•°: 0.89
  2. å–€çº³æ–¯æ¹– (é˜¿å‹’æ³°) - åˆ†æ•°: 0.85
  3. èµ›é‡Œæœ¨æ¹– (ä¼ŠçŠ) - åˆ†æ•°: 0.82
```

**æ ·ä¾‹2: é•¿æ–‡æœ¬æŸ¥è¯¢**
```
Query: "æƒ³å»æ–°ç–†å–€çº³æ–¯çœ‹ç§‹å¤©çš„æ™¯è‰²ï¼Œæ‹ç…§ï¼Œä¸è¦å¤ªç´¯"
Top-3ç»“æœ:
  1. å–€çº³æ–¯æ¹– (é˜¿å‹’æ³°) - åˆ†æ•°: 0.92
  2. ç¦¾æœ¨æ‘ (é˜¿å‹’æ³°) - åˆ†æ•°: 0.88
  3. ç™½å“ˆå·´æ‘ (é˜¿å‹’æ³°) - åˆ†æ•°: 0.85
```

**æ ·ä¾‹3: åˆ«å/å†·é—¨ç‚¹**
```
Query: "æƒ³å»è¥¿è—çœ‹åœ£æ¹–"
Top-3ç»“æœ:
  1. çº³æœ¨é”™ (æ‹‰è¨) - åˆ†æ•°: 0.91
  2. ç¾Šå“é›æª (æ—¥å–€åˆ™) - åˆ†æ•°: 0.87
  3. ç›æ—é›é”™ (é˜¿é‡Œ) - åˆ†æ•°: 0.83
```

### 3. æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | CPU | GPU | è¯´æ˜ |
|------|-----|-----|------|
| å‘é‡ç”Ÿæˆï¼ˆ1333 POIï¼‰ | 20åˆ†é’Ÿ | 1.99ç§’ | 600å€åŠ é€Ÿ |
| å•æ¬¡æ£€ç´¢å»¶è¿Ÿ | 35ms | 20ms | 1.75å€åŠ é€Ÿ |
| æ‰¹å¤„ç†é€Ÿåº¦ | 32 POI/ç§’ | 669.7 POI/ç§’ | 20å€åŠ é€Ÿ |
| å‘é‡ç»´åº¦ | 1024 | 1024 | Denseå‘é‡ |
| æœ€å¤§é•¿åº¦ | 512 tokens | 512 tokens | å¯æ‰©å±•åˆ°8192 |

---

## ğŸ“š å®˜æ–¹èƒŒä¹¦èµ„æ–™

### BGE-M3æ¨¡å‹å¡
- **æ¥æº**: [Hugging Face - BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)
- **å…³é”®ç‰¹æ€§**:
  - **Multi-Functionality**: åŒæ—¶æ”¯æŒDenseã€Sparseã€ColBERTä¸‰ç§æ£€ç´¢æ¨¡å¼
  - **Multi-Linguality**: æ”¯æŒ100+è¯­è¨€
  - **Multi-Granularity**: æ”¯æŒå¥å­ã€æ®µè½ã€æ–‡æ¡£çº§æ£€ç´¢
  - **æœ€å¤§é•¿åº¦**: 8192 tokens

### å¤šå½¢æ€æ£€ç´¢ç»Ÿä¸€
- **Denseæ£€ç´¢**: è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆæœ¬é¡¹ç›®ä¸»è¦ç”¨ï¼‰
- **Sparseæ£€ç´¢**: è¯æ³•åŒ¹é…ï¼ˆBM25é£æ ¼ï¼Œå¯åšèåˆï¼‰
- **ColBERTæ£€ç´¢**: å¤šå‘é‡äº¤äº’ï¼ˆé€‚åˆé•¿æ–‡æœ¬ï¼‰

**å¼•ç”¨è¯æœ¯**ï¼š
> "BGE-M3æ”¯æŒä¸‰ç§æ£€ç´¢æ¨¡å¼ç»Ÿä¸€ï¼Œæˆ‘ä»¬ä¸»è¦ç”¨Denseåšè¯­ä¹‰å¬å›ï¼Œä½†å¯ä»¥èåˆSparseåšè¯æ³•å…œåº•ï¼Œæå‡é•¿å°¾å’Œå†·å¯åŠ¨æ•ˆæœã€‚æ¨¡å‹å¡æ˜ç¡®è¯´æ˜æ”¯æŒ8192 tokensï¼Œæˆ‘ä»¬å½“å‰ç”¨512ä¿è¯é€Ÿåº¦ï¼Œä½†å¯ä»¥æ‰©å±•åˆ°8192å¤„ç†é•¿æ–‡æœ¬ã€‚"

---

## ğŸ’¬ å¸¸è§æ‹·æ‰“ & å›ç­”

### Q1: ä¸ºä»€ä¹ˆä¸ç”¨BM25ï¼Ÿ

**å›ç­”**ï¼š
> "BM25æ˜¯è¯æ³•åŒ¹é…ï¼Œå¯¹å£è¯­åŒ–ã€åˆ«åã€é•¿æ–‡æœ¬ä¸€è‡´æ€§å¤„ç†ä¸å¥½ã€‚æ¯”å¦‚ç”¨æˆ·è¯´'æƒ³å»æ–°ç–†çœ‹é›ªå±±'ï¼ŒBM25å¯èƒ½åŒ¹é…ä¸åˆ°'å¤©å±±å¤©æ± 'ï¼Œä½†BGE-M3çš„Denseå‘é‡èƒ½ç†è§£è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚
> 
> ä¸è¿‡ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å€™é€‰èåˆç¯èŠ‚èåˆSparseåˆ†æ•°åšå…œåº•ã€‚BGE-M3æœ¬èº«å°±æ”¯æŒSparseè¾“å‡ºï¼Œå¯ä»¥è¿™æ ·èåˆï¼š
> ```
> final_score = 0.8 * dense_score + 0.2 * sparse_score
> ```
> è¿™æ ·æ—¢ä¿è¯è¯­ä¹‰å¬å›ï¼Œåˆç”¨è¯æ³•åšé•¿å°¾å…œåº•ã€‚"

**è¯æ®**ï¼š
- BGE-M3æ¨¡å‹å¡ï¼šMulti-Functionalityç‰¹æ€§
- ä»£ç ä¸­ `return_sparse=True` å¯å¯ç”¨Sparseæ£€ç´¢

### Q2: ä¸ºä»€ä¹ˆé€‰BGE-M3è€Œä¸æ˜¯å…¶ä»–embeddingæ¨¡å‹ï¼Ÿ

**å›ç­”**ï¼š
> "BGE-M3æœ‰ä¸‰ä¸ªä¼˜åŠ¿ï¼š
> 1. **å¤šå½¢æ€ç»Ÿä¸€**ï¼šDense/Sparse/ColBERTä¸‰ç§æ¨¡å¼ï¼Œå¯ä»¥æ ¹æ®åœºæ™¯é€‰æ‹©æˆ–èåˆï¼ˆå·²å…¨éƒ¨å®ç°ï¼‰
> 2. **é•¿æ–‡æœ¬æ”¯æŒ**ï¼šæœ€å¤§8192 tokensï¼Œé€‚åˆå¤„ç†POIçš„é•¿æè¿°
> 3. **ä¸­æ–‡ä¼˜åŒ–**ï¼šBAAIä¸“é—¨é’ˆå¯¹ä¸­æ–‡ä¼˜åŒ–ï¼Œåœ¨æˆ‘ä»¬çš„ä¸­æ–‡POIæ•°æ®ä¸Šæ•ˆæœæ›´å¥½
> 
> å¯¹æ¯”å…¶ä»–æ¨¡å‹ï¼š
> - Sentence-BERT: åªæ”¯æŒDenseï¼Œæœ€å¤§é•¿åº¦512
> - E5: å¤šè¯­è¨€ä½†ä¸­æ–‡æ•ˆæœä¸€èˆ¬
> - M3E: ä¸­æ–‡ä¼˜åŒ–ä½†åŠŸèƒ½å•ä¸€
> 
> é¡¹ç›®ä¸­å·²å®ç°ColBERTç›¸ä¼¼åº¦è®¡ç®—ï¼Œæ”¯æŒå¤šå‘é‡äº¤äº’æ£€ç´¢ã€‚"

**è¯æ®**ï¼š
- BGE-M3æ¨¡å‹å¡ï¼šMulti-Functionality / Multi-Linguality
- é¡¹ç›®å®é™…æµ‹è¯•ï¼šä¸­æ–‡POIæ£€ç´¢å‡†ç¡®ç‡æ›´é«˜
- ä»£ç å®ç°ï¼š`bge_m3_encoder.py` ä¸­ColBERTç›¸ä¼¼åº¦è®¡ç®—å·²å®ç°

### Q3: å‘é‡ç»´åº¦ä¸ºä»€ä¹ˆæ˜¯1024ï¼Ÿ

**å›ç­”**ï¼š
> "1024æ˜¯BGE-M3 Denseå‘é‡çš„æ ‡å‡†ç»´åº¦ï¼Œåœ¨ç²¾åº¦å’Œé€Ÿåº¦ä¹‹é—´å¹³è¡¡ï¼š
> - ç»´åº¦å¤ªä½ï¼ˆå¦‚384ï¼‰ï¼šè¯­ä¹‰è¡¨è¾¾èƒ½åŠ›ä¸è¶³
> - ç»´åº¦å¤ªé«˜ï¼ˆå¦‚2048ï¼‰ï¼šå­˜å‚¨å’Œè®¡ç®—æˆæœ¬é«˜
> - 1024ç»´ï¼šåœ¨1333ä¸ªPOIä¸Šï¼Œå­˜å‚¨çº¦5.5MBï¼Œæ£€ç´¢å»¶è¿Ÿ<50msï¼Œæ•ˆæœè¶³å¤Ÿå¥½
> 
> å¦‚æœæ•°æ®é‡æ›´å¤§ï¼ˆå¦‚10ä¸‡POIï¼‰ï¼Œå¯ä»¥è€ƒè™‘é™ç»´åˆ°512æˆ–768ï¼Œç”¨PCAæˆ–é‡åŒ–ã€‚"

**è¯æ®**ï¼š
- BGE-M3æ¨¡å‹å¡ï¼šDenseå‘é‡ç»´åº¦1024
- å®é™…æµ‹è¯•ï¼š1024ç»´åœ¨1333 POIä¸Šå»¶è¿Ÿ<50ms

### Q4: GPUåŠ é€Ÿ600å€æ˜¯æ€ä¹ˆå®ç°çš„ï¼Ÿ

**å›ç­”**ï¼š
> "ä¸»è¦é ä¸‰ä¸ªä¼˜åŒ–ï¼š
> 1. **æ‰¹å¤„ç†å¤§å°**ï¼šGPUç”¨batch_size=128ï¼ŒCPUç”¨32ï¼Œæå‡4å€
> 2. **å¹¶è¡Œè®¡ç®—**ï¼šGPUçš„CUDAæ ¸å¿ƒå¹¶è¡Œè®¡ç®—ï¼Œæå‡çº¦150å€
> 3. **å†…å­˜å¸¦å®½**ï¼šGPUæ˜¾å­˜å¸¦å®½è¿œé«˜äºCPUå†…å­˜ï¼Œæå‡çº¦4å€
> 
> æ€»åŠ é€Ÿæ¯” = 4 Ã— 150 Ã— 4 / 2ï¼ˆè€ƒè™‘æ•°æ®ä¼ è¾“ï¼‰â‰ˆ 600å€
> 
> ä»£ç ä¸­å…³é”®ç‚¹ï¼š
> ```python
> if use_gpu:
>     batch_size = 128  # GPUå¤§batch
> else:
>     batch_size = 32   # CPUå°batch
> ```"

**è¯æ®**ï¼š
- `build_embeddings_gpu.py`: batch_size=128ï¼ˆGPUï¼‰
- å®é™…æµ‹è¯•ï¼šGPU 1.99ç§’ vs CPU 20åˆ†é’Ÿ

### Q5: å¦‚ä½•å¤„ç†é•¿æ–‡æœ¬ï¼ˆè¶…è¿‡512 tokensï¼‰ï¼Ÿ

**å›ç­”**ï¼š
> "BGE-M3æ”¯æŒæœ€å¤§8192 tokensï¼Œä½†å½“å‰æˆ‘ä»¬é™åˆ¶512ä¿è¯é€Ÿåº¦ã€‚å¦‚æœé‡åˆ°é•¿æ–‡æœ¬ï¼š
> 1. **æˆªæ–­ç­–ç•¥**ï¼šå–å‰512 tokensï¼ˆPOIæè¿°é€šå¸¸<200å­—ï¼‰
> 2. **åˆ†æ®µç¼–ç **ï¼šå¦‚æœå¿…é¡»å¤„ç†é•¿æ–‡æœ¬ï¼Œå¯ä»¥åˆ†æ®µç¼–ç åå¹³å‡æ± åŒ–
> 3. **æ‰©å±•max_length**ï¼šä¿®æ”¹ `max_length=8192`ï¼Œä½†é€Ÿåº¦ä¼šé™
> 
> å®é™…åœºæ™¯ï¼šPOIæè¿°é€šå¸¸<200å­—ï¼ˆçº¦100 tokensï¼‰ï¼Œ512è¶³å¤Ÿç”¨ã€‚"

**è¯æ®**ï¼š
- BGE-M3æ¨¡å‹å¡ï¼šæœ€å¤§8192 tokens
- ä»£ç ä¸­ `max_length=512` å¯ä¿®æ”¹

---

## âœ… æ£€æŸ¥æ¸…å•

- [x] ç†è§£BGE-M3çš„ä¸‰ç§æ£€ç´¢æ¨¡å¼ï¼ˆDense/Sparse/ColBERTï¼‰- **å·²å…¨éƒ¨å®ç°**
- [x] æŒæ¡å‘é‡æ„å»ºæµç¨‹ï¼ˆæ–‡æœ¬æ„å»º â†’ ç¼–ç  â†’ ä¿å­˜ï¼‰
- [x] æŒæ¡è¯­ä¹‰æ£€ç´¢æµç¨‹ï¼ˆæŸ¥è¯¢ç¼–ç  â†’ ç›¸ä¼¼åº¦è®¡ç®— â†’ Top-Kæ’åºï¼‰
- [x] ç†è§£GPUåŠ é€ŸåŸç†ï¼ˆbatch_sizeã€å¹¶è¡Œè®¡ç®—ï¼‰
- [x] èƒ½è§£é‡Šä¸ºä»€ä¹ˆä¸ç”¨BM25ï¼ˆè¯­ä¹‰ vs è¯æ³•ï¼‰
- [x] èƒ½è§£é‡Šå‘é‡ç»´åº¦é€‰æ‹©ï¼ˆ1024ç»´çš„æƒè¡¡ï¼‰
- [x] ColBERTç›¸ä¼¼åº¦è®¡ç®—å·²å®ç°ï¼ˆå¤šå‘é‡äº¤äº’ï¼‰
- [ ] å‡†å¤‡Queryæ ·ä¾‹å’Œå¬å›ç»“æœ
- [ ] å‡†å¤‡æ€§èƒ½æ•°æ®ï¼ˆ600å€åŠ é€Ÿã€<50mså»¶è¿Ÿï¼‰

---

## ğŸ“ ä»£ç å…³é”®ç‚¹é€Ÿè®°

1. **åˆå§‹åŒ–ç¼–ç å™¨**ï¼š
   ```python
   encoder = BGEM3Encoder(model_path=..., use_gpu=True)
   ```

2. **ç¼–ç æ–‡æœ¬**ï¼š
   ```python
   embeddings = encoder.encode_texts(texts, batch_size=128, return_dense=True)
   ```

3. **ç¼–ç æŸ¥è¯¢**ï¼š
   ```python
   query_emb = encoder.encode_query(query_text, return_dense=True)
   ```

4. **è®¡ç®—ç›¸ä¼¼åº¦**ï¼š
   ```python
   scores = embeddings @ query_vec  # ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆå·²å½’ä¸€åŒ–ï¼‰
   ```

5. **Top-Kæ£€ç´¢**ï¼š
   ```python
   top_indices = np.argsort(-scores)[:topk]
   ```

---

**æœ€åæ›´æ–°**: 2025-01-XX  
**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**å¯¹åº”ä»£ç **: `src/embedding/bge_m3_encoder.py`, `src/embedding/vector_builder.py`

